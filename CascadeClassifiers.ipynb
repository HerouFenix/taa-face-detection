{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Detection using Cascade Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "Face Detection is a computer vision problem that aims to attempt to accurately identify human faces that may, or may not, be present in a given photograph or video. Many efforts have been directed at this problem since the early 2000's, with it having started as a subbranch of the problem of Object-Class Detection but having evolved into a main focus of machine learning algorithms due to its prevalence and paramountcy to the paradigm of Facial Recognition, another problem that is nowadays used in a wide-branch of technologies and biometrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Detection Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import time\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Cascade Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initCascade(path):\n",
    "    cascadeClassifier = cv2.CascadeClassifier()\n",
    "    \n",
    "    # Try to load the model\n",
    "    if not cascadeClassifier.load(cv2.samples.findFile(path)):\n",
    "        print(\"Error - Unable to load classifier\")\n",
    "        exit(0)\n",
    "    \n",
    "    return cascadeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectFaces(classifier, imgPath, scaleFactor = 1.1, minNeighbors = 3):\n",
    "    img = cv2.imread(imgPath)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Get the Faces\n",
    "    faces = classifier.detectMultiScale(\n",
    "        img,\n",
    "        scaleFactor,\n",
    "        minNeighbors,\n",
    "    )\n",
    "    \n",
    "    #print(faces)\n",
    "    \n",
    "    return faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawBoxes(imgPath, rects):\n",
    "    img = cv2.imread(imgPath)\n",
    "\n",
    "    # Draw the bounding boxes\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img, (x,y), (x+w, y+h), (0,0,255), 3)\n",
    "    \n",
    "    cv2.imshow(\"Detected Faces\", img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeResults(path, faces, filename):\n",
    "    with open(\"./CascadeClassifier/pred/\"+filename, \"a\") as writer:\n",
    "        #print(\"Writing results for image \" + path.split(\"/\")[1])\n",
    "        writer.write(path.split(\"/\")[-1].rstrip()+\"\\n\")\n",
    "        writer.write(str(len(faces))+\"\\n\")\n",
    "        \n",
    "        for (x,y,w,h) in faces:\n",
    "            faceCoords = str(x)+\" \"+str(y)+\" \"+str(w)+\" \"+str(h)\n",
    "            \n",
    "            writer.write(faceCoords+\"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_results(results_file, expected_file):\n",
    "    image_faces = {}\n",
    "    \n",
    "    with open(expected_file, \"r\") as reader:\n",
    "        while True:\n",
    "            image_dir = reader.readline()\n",
    "            if not image_dir: \n",
    "                break\n",
    "            image_faces[image_dir] = []\n",
    "            num_faces = int(reader.readline())\n",
    "            for i in range(num_faces):\n",
    "                image_faces[image_dir].append([int(elem) for elem in reader.readline().rstrip().split()])\n",
    "                \n",
    "    true_pos = 0\n",
    "    false_pos = 0\n",
    "    false_neg = 0\n",
    "    with open(results_file, \"r\") as reader:\n",
    "        while True:\n",
    "            image_dir = reader.readline()\n",
    "            \n",
    "            if not image_dir:\n",
    "                return true_pos, false_pos, false_neg\n",
    "            \n",
    "            expected_images = image_faces[image_dir]\n",
    "            num_found_faces = int(reader.readline())\n",
    "            \n",
    "            for i in range(num_found_faces):\n",
    "                start_x, start_y, width, height = [int(elem) for elem in reader.readline().split()]\n",
    "                ## In order to be recognized as a face, it has to be within bounds, therfore\n",
    "                compatible_index = -1\n",
    "                \n",
    "                for i in range(len(expected_images)):\n",
    "                    expected_face = expected_images[i]\n",
    "                    if start_x > expected_face[0] - 5 and start_y > expected_face[1] - 5 \\\n",
    "                    and start_x + width < expected_face[2] + 5 and start_y < expected_face[3] + 5:\n",
    "                        compatible_index = i\n",
    "                        break\n",
    "                        \n",
    "                if compatible_index != -1:\n",
    "                    true_pos += 1\n",
    "                    expected_images.pop(compatible_index)\n",
    "                else:\n",
    "                    false_pos += 1\n",
    "\n",
    "            false_neg += len(expected_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_bin_results(img_dir, results):\n",
    "    true_pos = 0\n",
    "    true_neg = 0\n",
    "    false_pos = 0\n",
    "    false_neg = 0\n",
    "    with open(results, \"r\") as reader:\n",
    "        while True:\n",
    "            image = reader.readline().rstrip()\n",
    "            if not image:\n",
    "                return true_pos, true_neg, false_pos, false_neg\n",
    "            num_found_faces = int(reader.readline())\n",
    "            if num_found_faces == 0:\n",
    "                if image[0:3] == \"Pos\": \n",
    "                    false_neg += 1\n",
    "                else:\n",
    "                    true_neg += 1\n",
    "            for i in range(num_found_faces):\n",
    "                start_x, start_y, width, height = [int(elem) for elem in reader.readline().split()]\n",
    "                if image[0:3] == \"Pos\": #It's a positive image, we check if width and height are closer to the image\n",
    "                    true_pos += 1\n",
    "                    #real_height, real_width, _ = cv2.imread(img_dir + image).shape\n",
    "                    #if abs(real_width - width) < 0.25*real_width and abs(real_height - height) < 0.25*real_height:\n",
    "                    #    true_pos += 1\n",
    "                    #else:\n",
    "                    #    false_pos += 1\n",
    "                else:\n",
    "                    false_pos += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Model - haarcascade_frontalface_default.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will be using one of OpenCV's pretrained models for cascade classifier face detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifier = initCascade(\"./CascadeClassifier/haarcascade_frontalface_default.xml\")\n",
    "classifier = initCascade(\"./CascadeClassifier/haarcascade_frontalface_default.xml\")\n",
    "\n",
    "#Get values for a specific image\n",
    "path = \"./ImageResources/Dataset/WIDER_train/images/0--Parade/0_Parade_Parade_0_178.jpg\"\n",
    "path2 = \"./ImageResources/Dataset/WIDER_train/images/4--Dancing/4_Dancing_Dancing_4_138.jpg\"\n",
    "path3 = \"./dg.jpg\"\n",
    "\n",
    "\n",
    "faces = detectFaces(classifier, path3,  minNeighbors = 6)\n",
    "drawBoxes(path3, faces)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 104.62989473342896 to finish results for the pretrained model stages\n",
      "Correctly found 1252\n",
      "Incorrectly found 3607\n",
      "Could not find 18753\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'compare_results_binary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-538d0512dc4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Could not find\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mtp_d2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtn_d2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp_d2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_d2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompare_results_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./CascadeClassifier/pred/noTrain.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Correctly found\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtp_d2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Correctly not found\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtn_d2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compare_results_binary' is not defined"
     ]
    }
   ],
   "source": [
    "# Get values for our Test Set\n",
    "\n",
    "path = \"./ImageResources/Dataset/WIDER_test/images\"\n",
    "\n",
    "classifier = initCascade(\"./CascadeClassifier/haarcascade_frontalface_default.xml\")\n",
    "\n",
    "results = open(\"./CascadeClassifier/pred/noTrain.txt\", \"w\")\n",
    "results.close()\n",
    "\n",
    "list_imgs = [join(path, f) for f in listdir(path) if isfile(join(path, f))]\n",
    "\n",
    "start = time.time()\n",
    "for img in list_imgs:\n",
    "    \n",
    "    faces = detectFaces(classifier, img)\n",
    "    writeResults(img,faces, \"noTrain.txt\")\n",
    "    \n",
    "end = time.time()\n",
    "print(\"It took \" + str(end - start) + \" to finish results for the pretrained model stages\")\n",
    "\n",
    "answers = \"./ImageResources/Dataset/WIDER_test/test.txt\"\n",
    "tp_d, fp_d, fn_d = compare_results(\"./CascadeClassifier/pred/noTrain.txt\", answers)\n",
    "print(\"Correctly found\", tp_d)\n",
    "print(\"Incorrectly found\", fp_d)\n",
    "print(\"Could not find\", fn_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 117.37798118591309 to finish results for the pretrained model stages\n",
      "Correctly found 835\n",
      "Correctly not found 9790\n",
      "Incorrectly found 249\n",
      "Could not find 9194\n"
     ]
    }
   ],
   "source": [
    "path = \"./ImageResources/Dataset/WIDER_test/cropped_images\"\n",
    "\n",
    "classifier = initCascade(\"./CascadeClassifier/haarcascade_frontalface_default.xml\")\n",
    "\n",
    "results = open(\"./CascadeClassifier/pred/noTrainBin.txt\", \"w\")\n",
    "results.close()\n",
    "\n",
    "list_imgs = [join(path, f) for f in listdir(path) if isfile(join(path, f))]\n",
    "\n",
    "start = time.time()\n",
    "for img in list_imgs:\n",
    "    \n",
    "    faces = detectFaces(classifier, img)\n",
    "    writeResults(img,faces, \"noTrainBin.txt\")\n",
    "    \n",
    "end = time.time()\n",
    "print(\"It took \" + str(end - start) + \" to finish results for the pretrained model stages\")\n",
    "\n",
    "tp_d2, tn_d2, fp_d2, fn_d2 = compare_bin_results(path, \"./CascadeClassifier/pred/noTrainBin.txt\")\n",
    "print(\"Correctly found\", tp_d2)\n",
    "print(\"Correctly not found\", tn_d2)\n",
    "print(\"Incorrectly found\", fp_d2)\n",
    "print(\"Could not find\", fn_d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 129.7256646156311 to finish results for the pretrained model stages\n",
      "It took 137.95983815193176 to finish results for the pretrained model stages\n",
      "It took 157.30901551246643 to finish results for the pretrained model stages\n",
      "It took 161.6300950050354 to finish results for the pretrained model stages\n",
      "It took 143.19811415672302 to finish results for the pretrained model stages\n",
      "It took 165.48867392539978 to finish results for the pretrained model stages\n",
      "It took 167.16038346290588 to finish results for the pretrained model stages\n",
      "It took 167.54135036468506 to finish results for the pretrained model stages\n",
      "It took 160.36675810813904 to finish results for the pretrained model stages\n"
     ]
    }
   ],
   "source": [
    "path = \"./ImageResources/Dataset/WIDER_test/images\"\n",
    "answers = \"./ImageResources/Dataset/WIDER_test/test.txt\"\n",
    "bin_path = \"./ImageResources/Dataset/WIDER_test/cropped_images\"\n",
    "\n",
    "tp_arr = []\n",
    "fp_arr = []\n",
    "fn_arr = []\n",
    "\n",
    "tp_arr_b = []\n",
    "tn_arr_b = []\n",
    "fp_arr_b = []\n",
    "fn_arr_b = []\n",
    "\n",
    "acc_arr = []\n",
    "recall_arr = []\n",
    "\n",
    "acc_arr_b = []\n",
    "recall_arr_b = []\n",
    "\n",
    "for neighbours in range(1,10):\n",
    "    classifier = initCascade(\"./CascadeClassifier/haarcascade_frontalface_default.xml\")\n",
    "    \n",
    "    results = open(\"./CascadeClassifier/pred/noTrain.txt\", \"w\")\n",
    "    results.close()\n",
    "\n",
    "    list_imgs = [join(path, f) for f in listdir(path) if isfile(join(path, f))]\n",
    "\n",
    "    start = time.time()\n",
    "    for img in list_imgs:\n",
    "\n",
    "        faces = detectFaces(classifier, img, minNeighbors=neighbours)\n",
    "        writeResults(img,faces, \"noTrain.txt\")\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"It took \" + str(end - start) + \" to finish results for the pretrained model stages\")\n",
    "    \n",
    "    tp, fp, fn = compare_results(\"./CascadeClassifier/pred/noTrain.txt\", answers)\n",
    "    \n",
    "    tp_arr.append(tp)\n",
    "    fp_arr.append(fp)\n",
    "    fn_arr.append(fn)\n",
    "    \n",
    "    results = open(\"./CascadeClassifier/pred/noTrainBin.txt\", \"w\")\n",
    "    results.close()\n",
    "\n",
    "    list_imgs = [join(bin_path, f) for f in listdir(bin_path) if isfile(join(bin_path, f))]\n",
    "\n",
    "    start = time.time()\n",
    "    for img in list_imgs:\n",
    "\n",
    "        faces = detectFaces(classifier, img, minNeighbors=neighbours)\n",
    "        writeResults(img,faces, \"noTrainBin.txt\")\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"It took \" + str(end - start) + \" to finish results for the pretrained model stages\")\n",
    "\n",
    "    tp, tn, fp, fn = compare_bin_results(path, \"./CascadeClassifier/pred/noTrainBin.txt\")\n",
    "    \n",
    "    tp_arr_b.append(tp)\n",
    "    tn_arr_b.append(tn)\n",
    "    fp_arr_b.append(fp)\n",
    "    fn_arr_b.append(fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1877, 1191, 835, 636, 509, 395, 316, 261, 222]\n",
      "[8987, 9561, 9790, 9882, 9920, 9945, 9960, 9975, 9984]\n",
      "[1486, 550, 249, 132, 87, 58, 43, 26, 17]\n",
      "[8303, 8880, 9194, 9377, 9494, 9603, 9680, 9735, 9774]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (9,) and (0,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d754defe2814>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtp_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"#e33641\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'minNeighbors'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'True Positives'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2761\u001b[0m     return gca().plot(\n\u001b[1;32m   2762\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2763\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1644\u001b[0m         \"\"\"\n\u001b[1;32m   1645\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1646\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1647\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    343\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (9,) and (0,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "print(tp_arr_b)\n",
    "print(tn_arr_b)\n",
    "print(fp_arr_b)\n",
    "print(fn_arr_b)\n",
    "\n",
    "\n",
    "plt.figure(figsize = (20,7))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(range(1,10), tp_arr, color=\"#e33641\")\n",
    "plt.xlabel('minNeighbors')\n",
    "plt.ylabel('True Positives')\n",
    "plt.title('True positives in function of the number of minimum required neighbours',fontsize=15)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(range(1,10), fp_arr, color=\"#731f58\")\n",
    "plt.xlabel('minNeighbors')\n",
    "plt.ylabel('False Positives')\n",
    "plt.title('False Positives in function of the number of minimum required neighbours',fontsize=15)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(1,10), fn_arr, color=\"#2a8281\")\n",
    "plt.xlabel('minNeighbors')\n",
    "plt.ylabel('False Negatives')\n",
    "plt.title('False Negatives in function of the number of minimum required neighbours',fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Scale Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 275.0609700679779 to finish results for the pretrained model stages\n",
      "It took 294.1935043334961 to finish results for the pretrained model stages\n",
      "It took 141.47845935821533 to finish results for the pretrained model stages\n",
      "It took 140.34717273712158 to finish results for the pretrained model stages\n",
      "It took 110.81240773200989 to finish results for the pretrained model stages\n",
      "It took 114.43330097198486 to finish results for the pretrained model stages\n",
      "It took 92.18318510055542 to finish results for the pretrained model stages\n",
      "It took 95.70916318893433 to finish results for the pretrained model stages\n",
      "It took 71.63717126846313 to finish results for the pretrained model stages\n",
      "It took 76.51829719543457 to finish results for the pretrained model stages\n"
     ]
    }
   ],
   "source": [
    "path = \"./ImageResources/Dataset/WIDER_test/images\"\n",
    "answers = \"./ImageResources/Dataset/WIDER_test/test.txt\"\n",
    "bin_path = \"./ImageResources/Dataset/WIDER_test/cropped_images\"\n",
    "\n",
    "tp_arr2 = []\n",
    "fp_arr2 = []\n",
    "fn_arr2 = []\n",
    "\n",
    "acc_arr2 = []\n",
    "recall_arr2 = []\n",
    "\n",
    "tp_arr2_b = []\n",
    "tn_arr2_b = []\n",
    "fp_arr2_b = []\n",
    "fn_arr2_b = []\n",
    "\n",
    "acc_arr2_b = []\n",
    "recall_arr2_b = []\n",
    "\n",
    "neighbours = 6 #Substitute for optimal number of neighbours\n",
    "for scale_factor in [1.05, 1.10, 1.15, 1.20, 1.25]:\n",
    "    classifier = initCascade(\"./CascadeClassifier/haarcascade_frontalface_default.xml\")\n",
    "    \n",
    "    results = open(\"./CascadeClassifier/pred/noTrain.txt\", \"w\")\n",
    "    results.close()\n",
    "\n",
    "    list_imgs = [join(path, f) for f in listdir(path) if isfile(join(path, f))]\n",
    "\n",
    "    start = time.time()\n",
    "    for img in list_imgs:\n",
    "\n",
    "        faces = detectFaces(classifier, img, scaleFactor=scale_factor)\n",
    "        writeResults(img,faces, \"noTrain.txt\")\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"It took \" + str(end - start) + \" to finish results for the pretrained model stages\")\n",
    "    \n",
    "    tp, fp, fn = compare_results(\"./CascadeClassifier/pred/noTrain.txt\", answers)\n",
    "    \n",
    "    tp_arr2.append(tp)\n",
    "    fp_arr2.append(fp)\n",
    "    fn_arr2.append(fn)\n",
    "    \n",
    "    results = open(\"./CascadeClassifier/pred/noTrainBin.txt\", \"w\")\n",
    "    results.close()\n",
    "\n",
    "    list_imgs = [join(bin_path, f) for f in listdir(bin_path) if isfile(join(bin_path, f))]\n",
    "\n",
    "    start = time.time()\n",
    "    for img in list_imgs:\n",
    "\n",
    "        faces = detectFaces(classifier, img, scaleFactor=scale_factor)\n",
    "        writeResults(img,faces, \"noTrainBin.txt\")\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"It took \" + str(end - start) + \" to finish results for the pretrained model stages\")\n",
    "\n",
    "    tp, tn, fp, fn = compare_bin_results(path, \"./CascadeClassifier/pred/noTrainBin.txt\")\n",
    "    \n",
    "    tp_arr2_b.append(tp)\n",
    "    tn_arr2_b.append(tn)\n",
    "    fp_arr2_b.append(fp)\n",
    "    fn_arr2_b.append(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true positives, complex test [1471, 1252, 1030, 931, 823]\n",
      "false positives, complex test [6434, 3607, 2867, 2337, 1801]\n",
      "false negatives, complex test [18534, 18753, 18975, 19074, 19182]\n",
      "true positives, simple test [1609, 835, 624, 468, 274]\n",
      "true negatives, simple test [9379, 9790, 9875, 9920, 9961]\n",
      "false positives, simple test [817, 249, 142, 89, 41]\n",
      "false negatives, simple test [8516, 9194, 9387, 9535, 9724]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (6,) and (5,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-071b556be4f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtp_arr2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"#e33641\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'scaleFactor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'True Positives'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2761\u001b[0m     return gca().plot(\n\u001b[1;32m   2762\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2763\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1644\u001b[0m         \"\"\"\n\u001b[1;32m   1645\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1646\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1647\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    343\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (6,) and (5,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"true positives, complex test\", tp_arr2)\n",
    "print(\"false positives, complex test\", fp_arr2)\n",
    "print(\"false negatives, complex test\",fn_arr2)\n",
    "\n",
    "print(\"true positives, simple test\",tp_arr2_b)\n",
    "print(\"true negatives, simple test\",tn_arr2_b)\n",
    "print(\"false positives, simple test\",fp_arr2_b)\n",
    "print(\"false negatives, simple test\",fn_arr2_b)\n",
    "\n",
    "plt.figure(figsize = (20,7))\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot([0.95, 1, 1.05, 1.10, 1.15, 1.20], tp_arr2, color=\"#e33641\")\n",
    "plt.xlabel('scaleFactor')\n",
    "plt.ylabel('True Positives')\n",
    "plt.title('True positives in function of the scale factor',fontsize=15)\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot([0.95, 1, 1.05, 1.10, 1.15, 1.20], fp_arr2, color=\"#731f58\")\n",
    "plt.xlabel('scaleFactor')\n",
    "plt.ylabel('False Positives')\n",
    "plt.title('False Positives in function of the scale factor',fontsize=15)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot([0.95, 1, 1.05, 1.10, 1.15, 1.20], fn_arr2, color=\"#2a8281\")\n",
    "plt.xlabel('scaleFactor')\n",
    "plt.ylabel('False Negatives')\n",
    "plt.title('False Negatives in function of the scale factor',fontsize=15)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (20,7))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot([0.95, 1, 1.05, 1.10, 1.15, 1.20], acc_arr2, color=\"#e33641\")\n",
    "plt.xlabel('scaleFactor')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy in function of the number of minimum required neighbours',fontsize=15)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot([0.95, 1, 1.05, 1.10, 1.15, 1.20], recall_arr2, color=\"#731f58\")\n",
    "plt.xlabel('scaleFactor')\n",
    "plt.ylabel('Recall')\n",
    "plt.title('Recall in function of the number of minimum required neighbours',fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First you have to run the following command:\n",
    "```\n",
    "opencv_createsamples -info ../ImageResources/Dataset/WIDER_train/cropped_images/positive_info.dat -num 10000 -w 32 -h 32 -vec faces.vec\n",
    "```\n",
    "Where positive_info.dat should be a file containing all positive images, how many faces there in each image and their bounding boxes in the following format:\n",
    "```\n",
    "image_path no_faces x y width height\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the classifier we need to run the following command:\n",
    "```\n",
    "opencv_traincascade -data model -vec faces.vec -bg negative_info.txt -numPos 2500 -numNeg 2500 -numStages 1 -w 32 -h 32 \n",
    "```\n",
    "\n",
    "Where model is a folder that we should create beforehand in which the model will be stored\n",
    "\n",
    "_minHitRate <min_hit_rate>_ : Minimal desired hit rate for each stage of the classifier. Overall hit rate may be estimated as (min_hit_rate ^ number_of_stages), [245] 4.1.\n",
    "\n",
    "_maxFalseAlarmRate <max_false_alarm_rate>_ : Maximal desired false alarm rate for each stage of the classifier. Overall false alarm rate may be estimated as (max_false_alarm_rate ^ number_of_stages), [245] 4.1.\n",
    "\n",
    "_weightTrimRate <weight_trim_rate>_ : Specifies whether trimming should be used and its weight. A decent choice is 0.95.\n",
    "\n",
    "_maxDepth <max_depth_of_weak_tree>_ : Maximal depth of a weak tree. A decent choice is 1, that is case of stumps.\n",
    "\n",
    "_maxWeakCount <max_weak_tree_count>_ : Maximal count of weak trees for every cascade stage. The boosted classifier (stage) will have so many weak trees (<=maxWeakCount), as needed to achieve the given -maxFalseAlarmRate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing number of stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detecting faces\n",
      "finished detecting faces\n",
      "finished showing image\n"
     ]
    }
   ],
   "source": [
    "#classifier = initCascade(\"./CascadeClassifier/models/model1stage/cascade.xml\")\n",
    "#classifier = initCascade(\"./CascadeClassifier/models/model5stages/cascade.xml\")\n",
    "classifier = initCascade(\"./CascadeClassifier/models/model0.9995hitrate/cascade.xml\")\n",
    "\n",
    "#Get values for a specific image\n",
    "path = \"./ImageResources/Dataset/WIDER_train/images/0--Parade/0_Parade_Parade_0_178.jpg\"\n",
    "path2 = \"./ImageResources/Dataset/WIDER_train/images/4--Dancing/4_Dancing_Dancing_4_138.jpg\"\n",
    "path3 = \"./dg.jpg\"\n",
    "path4= \"./ImageResources/Dataset/WIDER_train/images/27--Spa/27_Spa_Spa_27_37.jpg\"\n",
    "\n",
    "print(\"detecting faces\")\n",
    "faces = detectFaces(classifier, path,  minNeighbors = 6)\n",
    "print(\"finished detecting faces\")\n",
    "\n",
    "drawBoxes(path, faces)\n",
    "print(\"finished showing image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Images\n",
      "It took 7.718833923339844 to finish results for 10stages\n",
      "It took 1.0132694244384766 to finish results for 15stages\n",
      "It took 0.7829241752624512 to finish results for 20stages\n",
      "It took 0.8345327377319336 to finish results for 25stages\n",
      "It took 0.793405294418335 to finish results for 30stages\n",
      "It took 0.8399546146392822 to finish results for 35stages\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.2.0) /io/opencv/modules/core/src/utils/samples.cpp:62: error: (-2:Unspecified error) OpenCV samples: Can't find required data file: ./CascadeClassifier/models/model40stages/cascade.xml in function 'findFile'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3e67e5534c63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtimes_multi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mno_stages\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"10\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"15\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"20\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"25\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"30\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"35\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"40\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitCascade\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./CascadeClassifier/models/model\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mno_stages\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"stages/cascade.xml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Get values for our Test Set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-831b69f58ab6>\u001b[0m in \u001b[0;36minitCascade\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Try to load the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcascadeClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error - Unable to load classifier\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.2.0) /io/opencv/modules/core/src/utils/samples.cpp:62: error: (-2:Unspecified error) OpenCV samples: Can't find required data file: ./CascadeClassifier/models/model40stages/cascade.xml in function 'findFile'\n"
     ]
    }
   ],
   "source": [
    "path = \"./ImageResources/Dataset/wider_face_split/wider_face_val_bbx_gt.txt\" #Path to the test images\n",
    "\n",
    "print(\"Just one image\")\n",
    "times_single = []\n",
    "for no_stages in [\"5\", \"10\", \"15\", \"20\", \"25\", \"30\", \"35\", \"40\"]:\n",
    "    classifier = initCascade(\"./CascadeClassifier/models/model\"+no_stages+\"stages/cascade.xml\")\n",
    "\n",
    "    # Get values for our Test Set\n",
    "    path = \"./ImageResources/Dataset/WIDER_test/images\"\n",
    "\n",
    "    results = open(\"./CascadeClassifier/pred/\"+no_stages+\"stages.txt\", \"w\")\n",
    "    results.close()\n",
    "\n",
    "    list_imgs = [join(path, f) for f in listdir(path) if isfile(join(path, f))]\n",
    "    \n",
    "    start = time.time()\n",
    "    for img in list_imgs:\n",
    "        faces = detectFaces(classifier, img)\n",
    "        writeResults(img,faces, no_stages+\"stages.txt\")\n",
    "        break\n",
    "    end = time.time()\n",
    "    \n",
    "    times_single.append(end-start)\n",
    "    \n",
    "    print(\"It took \" + str(end - start) + \" to finish results for \" + no_stages + \"stages\")\n",
    "\n",
    "\n",
    "print(\"Multiple Images\")\n",
    "times_multi = []\n",
    "for no_stages in [\"10\", \"15\", \"20\", \"25\", \"30\", \"35\", \"40\"]:\n",
    "    classifier = initCascade(\"./CascadeClassifier/models/model\"+no_stages+\"stages/cascade.xml\")\n",
    "\n",
    "    # Get values for our Test Set\n",
    "    path = \"./ImageResources/Dataset/WIDER_test/images\"\n",
    "\n",
    "    results = open(\"./CascadeClassifier/pred/\"+no_stages+\"stages.txt\", \"w\")\n",
    "    results.close()\n",
    "\n",
    "    list_imgs = [join(path, f) for f in listdir(path) if isfile(join(path, f))]\n",
    "    \n",
    "    start = time.time()\n",
    "    for img in list_imgs:\n",
    "        faces = detectFaces(classifier, img)\n",
    "        writeResults(img,faces, no_stages+\"stages.txt\")\n",
    "        break\n",
    "    end = time.time()\n",
    "    times_multi.append(end-start)\n",
    "    \n",
    "    print(\"It took \" + str(end - start) + \" to finish results for \" + no_stages + \"stages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (8,) and (7,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-61735d8d4fc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"10\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"15\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"20\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"25\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"30\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"35\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"40\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimes_single\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"#e33641\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of Stages'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Time (seconds)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2761\u001b[0m     return gca().plot(\n\u001b[1;32m   2762\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2763\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1644\u001b[0m         \"\"\"\n\u001b[1;32m   1645\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1646\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1647\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    343\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (8,) and (7,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (20,7))\n",
    "    \n",
    "plt.subplot(1,2,1)\n",
    "plt.plot([\"5\", \"10\", \"15\", \"20\", \"25\", \"30\", \"35\", \"40\"], times_single, color=\"#e33641\")\n",
    "plt.xlabel('Number of Stages')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.title('Number of time it took to detect the faces on a single image per number of stages',fontsize=15)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot([\"10\", \"15\", \"20\", \"25\", \"30\", \"35\", \"40\"], times_multi, color=\"#731f58\")\n",
    "plt.xlabel('Number of Stages')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.title('Number of time it took to detect the faces on the Test set per number of stages',fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
